import json
import json_lines
import os
import random
from tqdm import tqdm
from nltk import sent_tokenize

def main(args):
    random.seed(42)
    os.makedirs(args.output_dir, exist_ok=True)
    filenames = [os.path.join(args.input_dir, f) for (_, _, fs) in os.walk(args.input_dir) for f in fs if 'keywords' in f]
    for filename in filenames:
        dataset_name = filename.split('/')[-2]
        data_split = filename.split('/')[-1].split('_')[-1].split('.')[0]
        output_file = os.path.join(args.output_dir, f"{filename.split('/')[-1].split('_')[-1]}")
        print(f'processing {dataset_name}: {filename} => {output_file}')
        cnt = 0
        with open(filename, 'rb') as fin, open(output_file, 'w', encoding='utf-8') as fout:
            for dial in tqdm(json_lines.reader(fin)):
                context = []
                turns_keywords = [turn['keywords'] for turn in dial]
                for i, turn in enumerate(dial):
                    if dataset_name == 'wikidialog':
                        # skip user turns that generated by T5 in wikidialog 
                        speaker = 'user' if i % 2 == 1 else 'system'
                    else:
                        speaker = 'user' if i % 2 == 0 else 'system'
                    utt = turn['utterance']
                    context_seq = '\n'.join([f"{turn['speaker']}: {turn['utt']}" for turn in context]+[f'{speaker}: '])
                    context.append({'speaker': speaker, 'utt': utt})
                    if i == 0 or (dataset_name == 'wikidialog' and speaker == 'user'):
                        continue
                    
                    if args.mode == 'rg':
                        input_seq = f'generate a response: all knowledge: | | context:\n\n{context_seq}'
                        fout.write(json.dumps({
                            'dataset': dataset_name,
                            'source': input_seq, 
                            'target': utt
                            }, ensure_ascii=False)+'\n')
                        continue

                    if args.mode == 'key2gen':
                        random.shuffle(turn['keywords'])
                        for j in range(len(turn['keywords'])):
                            random.shuffle(turn['keywords'][j])
                        keywords = ' | '.join([' : '.join(sent_keywords) for sent_keywords in turn['keywords']])
                        input_seq = f'generate a response: grounded knowledge: | {keywords} | context:\n\n{context_seq}'
                        json2dump = {
                            'dataset': dataset_name,
                            'source': input_seq, 
                            'target': utt
                            }
                        if data_split == 'validation':
                            json2dump.update({'keywords': turn['keywords']})
                        fout.write(json.dumps(json2dump, ensure_ascii=False)+'\n')
                        continue

                    if args.mode == 'key2gen_noisy':
                        possible_keywords_sents = turn['keywords'][:]
                        num_possible_keywords_turns = min(random.randint(1, 5), len(turns_keywords) - 1)
                        for turn_keywords in random.sample(turns_keywords[:i] + turns_keywords[i+1:], num_possible_keywords_turns):
                            possible_keywords_sents.extend(turn_keywords)
                        random.shuffle(possible_keywords_sents)
                        possible_keywords = ' | '.join([' : '.join(sent_keywords) for sent_keywords in possible_keywords_sents])
                        input_seq = f'generate a response: all knowledge: | {possible_keywords} | context:\n\n{context_seq}'
                        json2dump = {
                            'dataset': dataset_name,
                            'source': input_seq, 
                            'target': utt
                            }
                        if data_split == 'validation':
                            json2dump.update({'keywords': turn['keywords'], 'all_keywords': possible_keywords_sents})
                        fout.write(json.dumps(json2dump, ensure_ascii=False)+'\n')
                        continue
    

if __name__ == '__main__':
    from argparse import ArgumentParser
    parser = ArgumentParser(description="calculate NLU metrics for unified datasets")
    parser.add_argument('--input_dir', '-i', type=str, help='path to the input files')
    parser.add_argument('--output_dir', '-o', type=str, help='path to the output files')
    parser.add_argument('--mode', '-m', type=str, choices=['rg', 'key2gen', 'key2gen_noisy'], help='which task to perform')
    args = parser.parse_args()
    print(args)
    main(args)
